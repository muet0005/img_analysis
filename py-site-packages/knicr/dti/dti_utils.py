import os as os
import sys as sys
import csv as csv
import numpy as np
import nibabel as nb
from nibabel import nifti1 as nii
import nipype.interfaces.fsl as fsl
import socket as socket
import datetime as datetime
import argparse as argparse


__author__ = "Ryan Muetzel"
__license__ = "GPL"
__version__ = "0.1"



def write_log(cmd, oFile):
    """
    write_log
    Simple function to track data provenance via the nipype package
    
    Arguments:
    cmd: usually the nipype function.cmdline
    
    oFile: the file you want to write the logging to.
    """
    if os.path.exists(oFile):
        f = open(oFile, 'a')
    else:
        f = open(oFile, 'w')
    f.write('#######--------NEW INSTANCE CALLED--------#######' + '\n')
    f.write('timestamp=' + datetime.datetime.now().strftime('%d%b%Y_%H:%M:%S') + '\n')
    f.write('user=' + os.getlogin() + '\n')
    f.write('pwd=' + os.getcwd() + '\n')
    f.write('domainname=' + socket.getfqdn() + '\n')
    f.write('hostname=' + socket.gethostname() + '\n')
    f.write('osUname=' + str(os.uname()) + '\n')
    f.write('PythonVersion=' + sys.version + '\n')
    f.write(cmd + '\n')
    f.write('#######--------END OF LOG--------#######' + '\n')
    f.close()


def read_txt(txtFile):
    t = open(txtFile, 'r')
    raw_data = [] ; data = []
    csv_reader = csv.reader(t, delimiter=',')
    for line in csv_reader:
        raw_data.append(line[0])
    num_rows = len(raw_data)
    for i in range(0, num_rows):
        if not i == 0:
            data.append(raw_data[i])
    t.close()
    return data


class knicrDTIFIT():
    """
    kncirDTIFIT
    
    A class to fit the diffusion tensor using FSLs dti fit package
    
    ... more to come ...
    """
    def __init__(self, dtiDir):
        self.dtiDir = dtiDir
    
    def FSLdtifit(self, subject, **kwargs):
        tracula_format = True
        if not tracula_format:
            return
        #set the output base name
        obn = os.path.join(self.dtiDir, subject, 'dmri', 'fsl', 'dtifit_fsl')
        #specify the gradient table
        bvecs = os.path.join(self.dtiDir, subject, 'dmri', 'bvecs')
        #specify the bvalues
        bvals = os.path.join(self.dtiDir, subject, 'dmri', 'bvals')
        #give the dwi input file
        iFile = os.path.join(self.dtiDir, subject, 'dmri', 'data.nii.gz')
        #specify the brain mask
        mask = os.path.join(self.dtiDir, subject, 'dmri', 'fsl', 'nodif_brain_mask.nii.gz')
        #in case the brain mask doesn't exist, specify the output for BET
        nodif_brain = os.path.join(self.dtiDir, subject, 'dmri', 'fsl', 'nodif_brain.nii.gz')
        if not os.path.exists(os.path.dirname(obn)):
            #make the output folder
            os.makedirs(os.path.dirname(obn))
        #if the brain mask doesn't exist...make it with bet
        if not os.path.exists(mask):
            bet = fsl.BET(frac=0.25, in_file=iFile, mask=True, out_file=nodif_brain)
            bet.run()
            logFile = os.path.join(os.path.dirname(mask), 'bet.log')
            write_log(bet.cmdline, logFile)
        #if the FA map doesn't exist....compute the tensor
        if not os.path.exists(obn + '_FA.nii.gz'):
            #in the current version of nipype...it doesn't handle the out_files properly, in terms of finding them
            #thus, the ignore_exception flag is activated..
            fit = fsl.DTIFit(base_name=obn, bvals=bvals, bvecs=bvecs, dwi=iFile, mask=mask, terminal_output='stream', ignore_exception=True)
            fit.run()
            logFile = obn + '.log'
            write_log(fit.cmdline, logFile)
    



class knicrTBSS():
    """
    knicrTBSS 
    
    A class of functions to run FSLs TBSS module in a stepwise fashion
    
    ...more to come...
    """
    def __init__(self, tbssDir, statsDir, template):
        #set some general paths to be used by all functions
        self.tbssDir = tbssDir
        self.faDir = os.path.join(self.tbssDir, 'FA')
        self.template = template
        if not os.path.exists(self.faDir):
            os.makedirs(self.faDir)
        self.statsDir = statsDir
        if self.tbssDir == self.statsDir:
            print 'Stats output folder CANNOT be the same as the TBSS dir...'
            print 'Please change the stats output folder...'
            sys.exit(0)
    
    def tbss_1_preproc(self, dtiDir, subject, **kwargs):
        """
        tbss_1_preproc
        
        An adaptation from FSLs tbss_1_preproc: $FSLDIR/bin/tbss_1_preproc
        
        dtiDir is the location of your DTI data. The default is to assume FreeSurfer's TRACULA
        has been run first. It will look for dtifit_FA.nii.gz in $dtiDir/subject/dmri
        
        !!!Changes from the FSL version!!!
        Some zero-padding is done in FSL, presumably to speed up the nonlinear registration. This is skipped here!
        """
        self.subject = subject
        self.pfix = 'idc'
        fa = False
        tracula_format = False
        #check the optional arguments
        for i in kwargs.keys():
            if i == 'fa':
                if kwargs[i] == 'NLLS':
                    fa = os.path.join(dtiDir, subject, 'dmri', 'NLLS', 'dti_NLLS_fa.nii.gz')
                #because tracula seems to be doing some strange masking (with the aseg/wmparc)
                #we may need to compute the FA map again ourselves, with our own mask
                #this is because of the erosion step in tbss_1. 
                #The tracula brain_mask has some holes in the middle of the brain, where there are vessels.
                #this causes major problems with the erosion.  Further, the susceptibility artifact also causes masking problems for tbss.
                elif kwargs[i] == 'fsl':
                    fa = os.path.join(dtiDir, subject, 'dmri', 'fsl', 'dtifit_fsl_FA.nii.gz')
                elif os.path.exists(kwargs[i]) and kwargs[i].endswith('.nii.gz'):
                    fa = kwargs[i]
            elif i == 'pfix':
                self.pfix = kwargs[i]
        #if you didn't specify any FA map, assume tracula format as default.
        if not fa:
            tracula_format=True
        #make sure you can even find a tracula fa map
        if tracula_format:
            fa = os.path.join(dtiDir, subject, 'dmri', 'dtifit_FA.nii.gz')
        #if not, exit out.
        if not os.path.exists(str(fa)):
            print 'Cannot locate FA map: '
            print 'dtiDir: ', dtiDir
            print 'Subject: ', subject
            print 'fa: ', fa
            return False
        #begin the tbss_1 preprocessing
        print 'Processing: ', subject
        #first we need to erode the FA map a bit, to get rid of the high signal voxels around the edges
        logFile = os.path.join(self.faDir, self.pfix + '_' + subject + '.tbss_1_preproc.log')
        self.fa = os.path.join(self.faDir, self.pfix + '_' + subject + '_FA.nii.gz')
        if not os.path.exists(self.fa):
            fslmaths = fsl.ImageMaths(in_file=fa, op_string='-ero -ero', out_file=self.fa, output_type='NIFTI_GZ', out_data_type='float')
            fslmaths.run()
            write_log(fslmaths.cmdline, logFile)
        #make a mask of the eroded file now
        #this actually is a boundary mask for FLIRT input weighting
        self.mask = self.fa.replace('.nii.gz', '_mask.nii.gz')
        if not os.path.exists(self.mask):
            fslmaths = fsl.ImageMaths(in_file=self.fa, op_string='-bin', out_file=self.mask, output_type='NIFTI_GZ', out_data_type='float')
            fslmaths.run()
            write_log(fslmaths.cmdline, logFile)
            op_str = '-dilD -dilD -sub 1 -abs -add ' + self.mask
            fslmaths = fsl.ImageMaths(in_file=self.mask, op_string=op_str, out_file=self.mask, output_type='NIFTI_GZ', out_data_type='char')
            fslmaths.run()
            write_log(fslmaths.cmdline, logFile)
        return True
    
    def tbss_2_reg(self):
        """
        tbss_2_reg
        
        Tool to run the flirt and fnirt steps of tbss.
        
        Note: we skip the computation of the mean deformation (i.e., _warp.msf, for determining the best fit from the all-to-all registration)
        
        This step must always follow the tbss_1_preproc....the fa map determined in step one is passed as self.fa.
        If tbss_1 has already been run once, and it's run again, it will not overwrite...
        """
        config='FA_2_FMRIB58_1mm'
        if not os.path.exists(self.fa):
            print 'Fa map not found: ', self.fa
            print 'run tbss_1_preproc first...'
            return
        obn = os.path.join(self.faDir, os.path.basename(self.fa).replace('.nii.gz', '') + '_to_' + os.path.basename(self.template).replace('.nii.gz', '') + '_lin')
        oFile = obn + '.nii.gz'
        mat = obn + '.mat'
        logFile = os.path.join(self.faDir, 'idc_' + self.subject + '.tbss_2_reg.log')
        if not os.path.exists(oFile):
            flirt = fsl.FLIRT(in_file=self.fa, reference=self.template, out_file=oFile, in_weight=self.mask, out_matrix_file=mat)
            flirt.run()
            write_log(flirt.cmdline, logFile)
        obn = os.path.join(self.faDir, os.path.basename(self.fa).replace('.nii.gz', '') + '_to_' + os.path.basename(self.template).replace('.nii.gz', '') + '_nonlin')
        self.warpedFile = obn + '.nii.gz'
        self.warp = obn + '_warp.nii.gz'
        log = obn + '_log.txt'
        if not os.path.exists(self.warp):
            fnirt = fsl.FNIRT(in_file=self.fa, fieldcoeff_file=self.warp, warped_file=self.warpedFile, affine_file=mat, config_file=config, ref_file=self.template, log_file=log)
            fnirt.run()
            write_log(fnirt.cmdline, logFile)
    
    def tbss_3_postregA(self):
        """
        tbss_3_postregA (A= only applywarp step)
        
        This simply does the applywarp step from tbss_3.
        This skips some main steps...namely the fsl merge and the skeletonization. Those are done in two subsequent steps...
        """
        if not os.path.exists(self.warp):
            print 'Cannot find warp file: ', self.warp
            return
        applywarp = fsl.ApplyWarp(in_file=self.fa, field_file=self.warp, out_file=self.warpedFile, ref_file=self.template, relwarp=True)
        applywarp.run()
        logFile = os.path.join(self.faDir, 'idc_' + self.subject + '.tbss_3_postreg.log')
        write_log(applywarp.cmdline, logFile)
    
    def tbss_3_postregB(self, subjList, merged4d, **kwargs):
        self.threshold = 0.2 #pass a kwarg to change it?
        #first read in the subject list if it's not already in txt format.
        if not isinstance(subjList, list):
            txtopts = ('.csv', '.txt')
            if subjList.endswith(txtopts):
                subjList = read_txt(subjList)
        self.subjList = subjList
        mergeList = []
        #go through the subjlist and make a list of file names to append
        for subj in subjList:
            fa = os.path.join(os.path.join(self.faDir, self.pfix + '_' + subj + '_FA_to_' + os.path.basename(self.template).replace('.nii.gz', '') + '_nonlin.nii.gz'))
            if os.path.exists(fa):
                mergeList.append(fa)
            else:
                #error check....if the fa map isn't there...the whole show stops...
                print 'Subject: ', subj, '  Cannot locate FA MAP!!'
                print 'Cannot continue!!!'
                return False
        #do another sanity check on the 4d output.
        #make sure it's going into a unique folder for whoever is running the code to avoid multiple ppl writing the same 4d
        if not os.path.dirname(merged4d) == self.statsDir:
            print 'Problem with 4D dirname...resetting the 4d file to the self.statsDir variable:'
            print 'Original 4d: ', merged4d
            merged4d = os.path.join(self.statsDir, os.path.basename(merged4d))
            print 'New 4d: ', merged4d
        fslmerge = fsl.Merge(dimension='t', in_files=mergeList, merged_file=merged4d)
        fslmerge.run()
        logFile = os.path.join(self.statsDir, 'tbss_3_postregB.log')
        write_log(fslmerge.cmdline, logFile)
        #next we make the mean FA image
        op_str = '-max 0 -Tmin -bin'
        fslmaths = fsl.ImageMaths(in_file=merged4d, op_string=op_str, out_file=os.path.join(self.statsDir, 'mean_FA_mask.nii.gz'), output_type='NIFTI_GZ', out_data_type='char')
        fslmaths.run()
        write_log(fslmaths.cmdline, logFile)
        op_str = '-mas ' + os.path.join(self.statsDir, 'mean_FA_mask.nii.gz')
        fslmaths = fsl.ImageMaths(in_file=merged4d, op_string=op_str, out_file=merged4d, output_type='NIFTI_GZ')
        fslmaths.run()
        write_log(fslmaths.cmdline, logFile)
        op_str = '-Tmean'
        fslmaths = fsl.ImageMaths(in_file=merged4d, op_string=op_str, out_file=os.path.join(self.statsDir, 'mean_FA.nii.gz'), output_type='NIFTI_GZ')
        fslmaths.run()
        write_log(fslmaths.cmdline, logFile)
        #now make the initial skeleton
        tbss_skeleton = fsl.TractSkeleton(in_file=os.path.join(self.statsDir, 'mean_FA.nii.gz'), skeleton_file=os.path.join(self.statsDir, 'mean_FA_skeleton.nii.gz'))
        tbss_skeleton.run()
        write_log(tbss_skeleton.cmdline, logFile)
        self.merged4d = merged4d
        return True
    
    def tbss_4_prestats(self, **kwargs):
        threshold = 0.2 #use a kwarg to edit this?
        logFile = os.path.join(self.statsDir, 'tbss_4_prestats.log')
        print 'Creating Skeleton Mask using threshold', str(threshold)
        op_str = '-thr ' + str(threshold) + ' -bin'
        fslmaths = fsl.ImageMaths(in_file=os.path.join(self.statsDir, 'mean_FA_skeleton.nii.gz'), op_string=op_str, out_file=os.path.join(self.statsDir, 'mean_FA_skeleton_mask.nii.gz'), output_type='NIFTI_GZ')
        fslmaths.run()
        write_log(fslmaths.cmdline, logFile)
        print 'Creating skeleton distancemap (for use in projection search)'
        op_str = '-mul 1 -add 1 -add ' + os.path.join(self.statsDir, 'mean_FA_skeleton_mask.nii.gz')
        fslmaths = fsl.ImageMaths(in_file=os.path.join(self.statsDir, 'mean_FA_mask.nii.gz'), op_string=op_str, out_file=os.path.join(self.statsDir, 'mean_FA_skeleton_mask_dst.nii.gz'), output_type='NIFTI_GZ')
        fslmaths.run()
        write_log(fslmaths.cmdline, logFile)
        distancemap = fsl.DistanceMap(in_file=os.path.join(self.statsDir, 'mean_FA_skeleton_mask_dst.nii.gz'), distance_map=os.path.join(self.statsDir, 'mean_FA_skeleton_mask_dst.nii.gz'))
        distancemap.run()
        write_log(distancemap.cmdline, logFile)
        #do a quick check on the cingulum file...make sure it's the right size!
        nb_template = nb.load(self.template)
        x = abs(nb_template.get_affine()[0][0])
        y = abs(nb_template.get_affine()[1][1])
        z = abs(nb_template.get_affine()[2][2])
        cing_file1mm = os.path.join(os.environ['FSLDIR'], 'data', 'standard', 'LowerCingulum_1mm.nii.gz')
        if x == 1 and y == 1 and z == 1:
            cing_file = cing_file1mm
        elif x == 2 and y == 2 and z == 2:
            cing_file = os.path.join(os.environ['FSLDIR'], 'data', 'standard', 'LowerCingulum_2mm.nii.gz')
            if not os.path.exists(cing_file):
                cing_file = os.path.join(self.statsDir, 'LowerCingulum_2mm.nii.gz')
                flirt = fsl.FLIRT(in_file=cing_file1mm, reference=os.path.join(os.environ['FSLDIR'], 'data', 'standard', 'MNI152_T1_2mm.nii.gz'), out_file=cing_file, args='-applyisoxfm 2')
                flirt.run()
                write_log(flirt.cmdline, logFile)
        print 'Projecting all FA data onto Skeleton'
        final4d = self.merged4d.replace('.nii.gz', '_skeletonized.nii.gz')
        tbss_skeleton = fsl.TractSkeleton(in_file=os.path.join(self.statsDir, 'mean_FA.nii.gz'), threshold=threshold, distance_map=os.path.join(self.statsDir, 'mean_FA_skeleton_mask_dst.nii.gz'), data_file=self.merged4d, projected_data=final4d)
        print tbss_skeleton.cmdline
        print tbss_skeleton.input_spec()
        tbss_skeleton.run()
        write_log(tbss_skeleton.cmdline, logFile)
    

        